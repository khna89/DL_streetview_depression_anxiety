{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOcXy2_xKkRH"
   },
   "source": [
    "# Setting the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UEdrGtJE5N4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import requests\n",
    "from random import randint, sample\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "import math\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkazGaxeGQbx"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "drive.mount( '/content/drive', force_remount=True )\n",
    "os.chdir(\"/content/drive/MyDrive/\") #change the path so that it leads to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define this to sample Google Street View Images: \n",
    "api_key = 'some_string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SJ2KYFuJLPoM",
    "outputId": "fa8ecbe6-df64-4872-b31e-9da48409f6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-0.9.8-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
      "Collecting huggingface-hub (from timm)\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors (from timm)\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "Installing collected packages: safetensors, huggingface-hub, timm\n",
      "Successfully installed huggingface-hub-0.18.0 safetensors-0.4.0 timm-0.9.8\n"
     ]
    }
   ],
   "source": [
    "!pip install timm #you need to install it every time in the cloud to work with DeiT models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first download the data for neighbourhoods (\"buurten\") from the website of the Dutch Health Monitor (there is a link to Statline there, use that one, you don't need to submit a request, the data is openly available), only for variables Moderate to high risk of depression and anxiety and High risk of depression and anxiety. I used the results for adults and elderly from 2020. The following is done with this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivm = pd.read_csv('./NL_gezondheidsdata/rivm_depri_buurt_dataportal.csv', delimiter=';')\n",
    "display(rivm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivm = rivm.drop(['Marges','Leeftijd', 'Perioden'],axis = 1) #they are all the same\n",
    "rivm = rivm[rivm['WijkenEnBuurten'].str.contains('BU',case = False)] #we only use \"buurten\"\n",
    "rivm = rivm.drop(['ID'],axis = 1)\n",
    "rivm.columns = ['nbhood_code','moderate_high_risk','high_risk'] #only need these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I live in Den Bosch ('s-Hertogenbosch) and for sanity check I choose to display all the neighborhoods of DB because I know some of them, and also I'm curious. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_code = GM_WK_BU.ID[GM_WK_BU.Title.str.contains('hertogenbo', case = False)].item().strip()\n",
    "buurten_db = rivm[rivm.WijkenEnBuurten.str.contains(db_code[2:],case = False)]\n",
    "buurten_db = buurten_db.merge(GM_WK_BU[['ID', 'Title']], left_on='WijkenEnBuurten', right_on='ID', how='left')\n",
    "display(buurten_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verified: these numbers correspond to the numbers here https://www.rivm.nl/media/smap/depressie.html?gemeente=%27s-Hertogenbosch, so it's all good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to explore the distribution:\n",
    "plt.hist(rivm.moderate_high_risk)\n",
    "#plt.title('Distribution of moderate and high risk of depression and anxiety, % of the population')\n",
    "plt.show()\n",
    "plt.figure(figsize=(600/120,800/120))\n",
    "plt.hist(rivm.high_risk, bins = 5)\n",
    "#plt.title('Distribution of neighborhoods, high risk of depression and anxiety',\n",
    " #        fontweight = 'bold',\n",
    "  #       fontsize = 14)\n",
    "plt.xticks(fontsize=14)  # Set x-axis tick names\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"High risk, % of the population\", fontsize = 19.5)\n",
    "plt.ylabel(\"N neighborhoods\", fontsize = 19.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('hist.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more exploration\n",
    "\n",
    "n_buurten = rivm.shape[0]\n",
    "print(n_buurten) #this is how many neighbourhoods are available\n",
    "max_moderate, min_moderate, mean_moderate, sd_moderate = \\\n",
    "max(rivm.moderate_high_risk), min(rivm.moderate_high_risk), rivm.moderate_high_risk.mean(), rivm.moderate_high_risk.std()\n",
    "max_high, min_high, mean_high, sd_high = \\\n",
    "max(rivm.high_risk), min(rivm.high_risk), rivm.high_risk.mean(), rivm.high_risk.std()\n",
    "\n",
    "print(\"Moderate to high risk values: \\nmax {:.2f}, \\nmin {:.2f}, \\\n",
    "\\nmean {:.2f}, \\nSD {:.2f}\".format(max_moderate, min_moderate, mean_moderate, sd_moderate))\n",
    "print(\"\\nHigh risk values: \\nmax {:.2f}, \\nmin {:.2f}, \\\n",
    "\\nmean {:.2f}, \\nSD {:.2f}\".format(max_high, min_high, mean_high, sd_high))\n",
    "\n",
    "print(\"\\nSpecial neighborhoods:\")\n",
    "print(\"Maximum moderate+high risk: \\\n",
    "{}\".format(GM_WK_BU.ShortTitle[GM_WK_BU.ID  == rivm.nbhood_code[rivm.moderate_high_risk == max_moderate].item()].item()))\n",
    "print(\"Maximum high risk: \\\n",
    "{}\".format(GM_WK_BU.ShortTitle[GM_WK_BU.ID  == rivm.nbhood_code[rivm.high_risk == max_high].item()].item()))\n",
    "print(\"Minimum moderate+high risk: \\\n",
    "{}\".format(GM_WK_BU.ShortTitle[GM_WK_BU.ID  == rivm.nbhood_code[rivm.moderate_high_risk == min_moderate].item()].item()))\n",
    "#print(\"Minimum high risk: \\\n",
    "#{}\".format(GM_WK_BU.ShortTitle[GM_WK_BU.ID  == rivm.nbhood_code[rivm.high_risk == min_high]]))\n",
    "\n",
    "plt.figure(figsize=(600/120,800/120))\n",
    "rivm.high_risk.plot.box()\n",
    "plt.xticks(ticks=[], labels=[])  # Set x-axis tick names\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"\", fontsize = 20)\n",
    "plt.ylabel(\"High risk, % of the population\", fontsize = 20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('article BeneLearn/box.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create intervals to discretize the targets\n",
    "range_high = max_high - min_high\n",
    "print(\"range high =\", range_high)\n",
    "num_intervals = 5\n",
    "\n",
    "#create intervals-boxes:\n",
    "interval_size = range_high / num_intervals\n",
    "intervals = [min_high + (i + 1) * interval_size\n",
    "             for i in range(num_intervals)]\n",
    "print(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivm = rivm.dropna(subset=['high_risk'])\n",
    "rivm = rivm.dropna(subset=['moderate_high_risk'])\n",
    "\n",
    "#create a new column representing the categorical levels based on the high_risk values\n",
    "rivm['level_high'] = pd.cut(rivm['high_risk'], bins=[-float('inf')] + intervals, labels=False) + 1\n",
    "\n",
    "#count occurrences of each level\n",
    "level_counts = rivm['level_high'].value_counts().sort_index()\n",
    "print(level_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample neighbourhoods per level for download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook aims at 10 000 sampling points (so 10 000 SVI). There are 5 levels, so ideally it should sample 2 000 images per level to get a balanced dataset of the desired size. However, only the first two levels have >= 2 000 neighbourhoods to sample from. For these two levels the sampling part of the code would have to sample 1 image p/neighbourhood, while for the next levels - > 1, and the size of the sample per neighbourhood will be different for every level. So in this part we split the dataset per level in order to proceed with the sampling this way. \n",
    "\n",
    "Also in practice sampling 2000 images from 7 neighbourhoods of the last level results in a biased sample: the images are often from almost the same viewpoint. In order to avoid that, the notebook merges levels 4 and 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate into 4 dataframes\n",
    "first = rivm[rivm.level_high == 1]\n",
    "second = rivm[rivm.level_high == 2]\n",
    "third = rivm[rivm.level_high == 3]\n",
    "fourth = rivm[rivm['level_high'].isin([4, 5])]\n",
    "\n",
    "#n_sample for the number of neighborhoods to be sampled per level\n",
    "n_sample = 2500 \n",
    "sampled_first = first.groupby('level_high').apply(lambda x: x.sample(n=n_sample, replace=False)).reset_index(drop=True)\n",
    "display(sampled_first)\n",
    "\n",
    "sampled_second = second.groupby('level_high').apply(lambda x: x.sample(n=n_sample, replace=False)).reset_index(drop=True)\n",
    "display(sampled_second)\n",
    "\n",
    "n_sample = 833\n",
    "sampled_third = third.groupby('level_high').apply(lambda x: x.sample(n=n_sample, replace=False)).reset_index(drop=True)\n",
    "display(sampled_third)\n",
    "\n",
    "#and we take all the 100+ neighbourhoods from levels 4 and 5\n",
    "\n",
    "# create nbhood codes to look for in the geopandas df later on\n",
    "codes_first = sampled_first.nbhood_code\n",
    "codes_second = sampled_second.nbhood_code\n",
    "codes_third = sampled_third.nbhood_code\n",
    "codes_fourth = fourth.nbhood_code\n",
    "\n",
    "#now specify the level of risk in a discretized way\n",
    "first['level_high'] = 1\n",
    "second['level_high'] = 2\n",
    "third['level_high'] = 3\n",
    "fourth['level_high'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and use the geographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the division into neighbourhoods published by the Dutch Central Statistics Bureau (https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/geografische-data/wijk-en-buurtkaart-2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace 'something/buurt_2020_v3.shp' with the actual path to your .shp file\n",
    "shapefile_path = 'something/buurt_2020_v3.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Print the first few rows to inspect the data\n",
    "print(gdf.head())\n",
    "\n",
    "# Plot the shapefile\n",
    "gdf.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we don't need 200 columns:\n",
    "gdf = gdf[['BU_CODE', 'BU_NAAM', 'WK_CODE', 'GM_CODE', 'GM_NAAM', 'POSTCODE', 'geometry']]\n",
    "\n",
    "#transform the coordinates to lat lon\n",
    "#determine the current CRS\n",
    "print(\"Original CRS:\", gdf.crs)\n",
    "\n",
    "# Convert the CRS from EPSG:28992 to WGS84 (EPSG:4326)\n",
    "gdf_latlon = gdf.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import the shape of the NL itself, so that the sampled neighborhoods don't float in the abyss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = unary_union(gdf_latlon.geometry)\n",
    "\n",
    "# Create a new GeoDataFrame with the super-polygon\n",
    "gdf_nl = gpd.GeoDataFrame({'geometry': [nl]}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Plot the country's border\n",
    "gdf_nl.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_first = gdf_latlon[gdf_latlon['BU_CODE'].isin(codes_first)]\n",
    "gdf_second = gdf_latlon[gdf_latlon['BU_CODE'].isin(codes_second)]\n",
    "gdf_third = gdf_latlon[gdf_latlon['BU_CODE'].isin(codes_third)]\n",
    "gdf_fourth = gdf_latlon[gdf_latlon['BU_CODE'].isin(codes_fourth)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the neighbourhoods you sampled\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "gdf_nl.plot(facecolor='none', edgecolor='gray', ax=ax)\n",
    "first.plot(ax=ax, color='pink')\n",
    "second.plot(ax=ax, color='green')\n",
    "third.plot(ax=ax, color='yellow')\n",
    "fourth.plot(ax=ax, color='blue')\n",
    "\n",
    "first_patch = mpatches.Patch(color='pink', label='very low')\n",
    "second_patch = mpatches.Patch(color='green', label='low')\n",
    "third_patch = mpatches.Patch(color='yellow', label='moderate')\n",
    "fourth_patch = mpatches.Patch(color='blue', label='high&very high')\n",
    "plt.legend(handles=[first_patch, second_patch, third_patch, fourth_patch], \n",
    "           bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title(\"Sampled neighborhoods\")\n",
    "plt.savefig(\"some_path.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample coordinates with images available, from GSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_street_view_exists(latitude, longitude, api_key):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/streetview/metadata?location={latitude},{longitude}&key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    metadata = response.json()\n",
    "    if metadata['status'] == 'OK' and metadata['copyright'] == '© Google':\n",
    "        image_date = metadata['date']\n",
    "        image_year, image_month = map(int, image_date.split('-'))\n",
    "        if 5 <= image_month <= 9: #specify the months for sampling here\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def generate_coordinates(polygon, num_points):\n",
    "    min_x, min_y, max_x, max_y = polygon.bounds\n",
    "    points = []\n",
    "    while len(points) < num_points:\n",
    "        random_point = Point(random.uniform(min_x, max_x), random.uniform(min_y, max_y))\n",
    "        if polygon.contains(random_point):\n",
    "            if check_street_view_exists(random_point.y, random_point.x, api_key):\n",
    "                points.append(random_point)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the same functions with statements for debug. It's handy to use these if there is a need to figure out where the sampling gets stuck. For example, there are very little or no SVI for a certain neighbourhood for the desired time period - these will help to detect that situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_street_view_exists(latitude, longitude, api_key):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/streetview/metadata?location={latitude},{longitude}&key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    metadata = response.json()\n",
    "    is_image = False\n",
    "    is_date = None\n",
    "    if metadata['status'] == 'OK' and metadata['copyright'] == '© Google':\n",
    "        is_image = True\n",
    "        image_date = metadata['date']\n",
    "        image_year, image_month = map(int, image_date.split('-'))\n",
    "        if image_month < 5 or image_month > 9: #to produce \"green\" data use this: 5 <= image_month <= 9\n",
    "            is_date = True\n",
    "            return [True, is_image, is_date]\n",
    "        else:\n",
    "            is_date = False\n",
    "    return [False, is_image, is_date]\n",
    "\n",
    "def generate_coordinates(polygon, num_points):\n",
    "    min_x, min_y, max_x, max_y = polygon.bounds\n",
    "    points = []\n",
    "    log = pd.DataFrame(columns = ['for_point', 'n_requests', 'no_SVI', 'wrong_date','unk_date'])\n",
    "    n_requests = 0\n",
    "    no_SVI = 0\n",
    "    wrong_date = 0\n",
    "    unk_date = 0\n",
    "    while len(points) < num_points:\n",
    "        random_point = Point(random.uniform(min_x, max_x), random.uniform(min_y, max_y))\n",
    "        if polygon.contains(random_point):\n",
    "            check = check_street_view_exists(random_point.y, random_point.x, api_key)\n",
    "            n_requests +=1\n",
    "            if not check[0]:\n",
    "                if not check[1]:\n",
    "                    no_SVI +=1\n",
    "                if check[2] is False:\n",
    "                    wrong_date +=1\n",
    "                if check[2] is None:\n",
    "                    unk_date +=1\n",
    "            if (no_SVI % 50 == 0 and no_SVI != 0) or (wrong_date % 50 == 0 and wrong_date != 0) or (unk_date % 50 == 0 and unk_date != 0):\n",
    "                print('no_SVI:', no_SVI, 'wrong_date:', wrong_date, 'unk_date:', unk_date)\n",
    "\n",
    "            '''\n",
    "            #you can also uncomment this part and comment the previous if statement\n",
    "            #this will stop the code from looking after there was no image available for 300 attempts,\n",
    "            #and return whatever points were found for that neighbourhood.\n",
    "            #use the appropriate code later to determine which neighbourhoods don't in fact have coordinates\n",
    "            if wrong_date == 300 or no_SVI == 300:\n",
    "                #print(\"300 is reached, switching to the next neighborhood.\")\n",
    "                return points \n",
    "            '''\n",
    "            if check[0]:\n",
    "                points.append(random_point)\n",
    "                log.loc[len(points)] = [len(points)] + [n_requests] + [no_SVI] + [wrong_date] + [unk_date]\n",
    "                n_requests = 0\n",
    "                no_SVI = 0\n",
    "                wrong_date = 0\n",
    "                unk_date = 0\n",
    "    display(log)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sampling might get stuck due to the unavailability of the data, one could process the dataset in chunks, saving the intermediate process. The functions for that are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for step-by-step coordinates saving:\n",
    "def process_gdf_chunk(gdf_chunk, output_file,n_per_nbhood):\n",
    "    gdf_chunk['rand_points'] = gdf_chunk.apply(lambda x: generate_coordinates(x['geometry'], n_per_nbhood), axis=1)\n",
    "    gdf_chunk.to_pickle(output_file)\n",
    "    \n",
    "def process_gdf_in_chunks(gdf, chunk_size, output_prefix,n_p_nbhood):\n",
    "    num_chunks = math.ceil(len(gdf) / chunk_size)\n",
    "    print(num_chunks, chunk_size, len(gdf))\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        print(start)\n",
    "        end = min((i + 1) * chunk_size, len(gdf))\n",
    "        print(end)\n",
    "        gdf_chunk = gdf.iloc[start:end]\n",
    "        print(len(gdf_chunk))\n",
    "        output_file = f\"{output_prefix}_chunk_{i}.pkl\"\n",
    "        process_gdf_chunk(gdf_chunk, output_file, n_p_nbhood)\n",
    "        print(f\"Processed chunk {i + 1}/{num_chunks}, saved to {output_file}\")\n",
    "\n",
    "chunk_size = 50 #or specify other size\n",
    "output_prefix = \"fourth_level\" #set the right prefix\n",
    "process_gdf_in_chunks(gdf_fourth, chunk_size, output_prefix, 20) #here we sample 20 points, adjust that per level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to restart later:\n",
    "chunk_size = 250\n",
    "start_chunk =  6# Set this to the first unprocessed chunk.\n",
    "output_prefix = \"_first_level\"\n",
    "num_chunks = math.ceil(len(gdf_first) / chunk_size)\n",
    "for i in range(start_chunk, num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, len(gdf_first))\n",
    "    gdf_chunk = gdf_first.iloc[start:end]\n",
    "    output_file = f\"{output_prefix}_chunk_{i}.pkl\"\n",
    "    process_gdf_chunk(gdf_chunk, output_file, 1)\n",
    "    print(f\"Processed chunk {i + 1}/{num_chunks}, saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point all the coordinates for randpoints are saved in the chunks. They are checked for the availability of SVI within the certain time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to merge:\n",
    "def merge_chunks_to_gdf(gdfs, num_chunks, output_prefix):\n",
    "    for i in range(*num_chunks):\n",
    "        chunk_name = f\"{output_prefix}_chunk_{i}.pkl\"\n",
    "        chunk_data = pd.read_pickle(chunk_name)\n",
    "        gdfs.append(chunk_data) \n",
    "    return gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
    "\n",
    "merged = merge_chunks_to_gdf([], (0,20), output_prefix) \n",
    "\n",
    "#check if everything went right after you maybe restarted from some chunk:\n",
    "merged = merged.drop_duplicates(subset='BU_CODE', keep='first')\n",
    "print(merged['BU_CODE'].nunique()) #should be the amount of neighbourhoods you sampled\n",
    "display(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augment it with depression and anxiety scores and levels\n",
    "merged = merged.merge(rivm, left_on='BU_CODE', right_on='nbhood_code', how='left')\n",
    "merged = merged.drop(['moderate_high_risk'],axis = 1)\n",
    "\n",
    "#save, load, re-create:\n",
    "merged.to_pickle(\"merged_with_randpoints.pkl\")\n",
    "df=pd.read_pickle('merged_with_randpoints.pkl')\n",
    "gdf_df = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "display(gdf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To load the saved geodataframes later:\n",
    "\n",
    "df = pd.read_pickle('whatever_gdf_you_saved.pkl')\n",
    "\n",
    "gdf_4_levels = gpd.GeoDataFrame(df, geometry='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to deal with the non-existence of SVI for a certain neighbourhood for a certain timeframe. I've tried to augment functions with print statements (available above) to see when it can't find the coordinates and for what reason. For one neighbourhood for one of the datasets (I collected data separately for \"greenery\" and \"no greenery\" months) it couldn't find any images for 100 000 attempts. So then it was obvious I have to skip one of the neighbourhoods, and then I had to deal with it, because then there were only 6 neighbourhoods available in the \"very high\" category, so I ended up merging this category with the previous one. What I found more useful was letting it save whatever it managed to find after some threshold number of attempts, and then check with the code below, how many neighourhoods lack how many coordinates. This approach bears a risk of not finding something that is there, but in general this is faster, and also if the differences between categories are not very big after skipping some neighbourhoods, then one could just proceed with the nearly balanced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_rows = merged[merged['rand_points'].isnull()]\n",
    "display(none_rows)\n",
    "none_rows_level_high_counts = none_rows['level_high'].value_counts()\n",
    "print(\"\\nCounts in none_rows DataFrame:\")\n",
    "print(none_rows_level_high_counts)\n",
    "\n",
    "#drop the rows with None from merged\n",
    "merged = merged[merged['rand_points'].notnull()]\n",
    "print(len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample more\n",
    "selected_nbhoods = pd.DataFrame(columns=rivm.columns)\n",
    "\n",
    "for lev, count in none_rows_level_high_counts.items():\n",
    "    all_that_level = rivm[rivm.level_high == lev]\n",
    "    added = 0\n",
    "    while added < count:\n",
    "        selected = all_that_level.iloc[randint(0,len(all_that_level)-1)]\n",
    "        if not selected.nbhood_code in gdf_df.BU_CODE.values:\n",
    "            selected_nbhoods = pd.concat([selected_nbhoods, selected.to_frame().T], ignore_index=True)\n",
    "            added += 1\n",
    "\n",
    "gdf_selected_nbhoods = gdf[gdf['BU_CODE'].isin(selected_nbhoods['nbhood_code'])]\n",
    "gdf_selected_nbhoods = gdf_selected_nbhoods[['BU_CODE', 'BU_NAAM', 'WK_CODE', 'GM_CODE', 'GM_NAAM', 'POSTCODE', 'geometry']]\n",
    "gdf_selected_nbhoods = gdf_selected_nbhoods.to_crs(epsg=4326)\n",
    "#not sure for what I used the line below:\n",
    "#gdf_selected_nbhoods = gdf_selected_nbhoods.merge(selected_nbhoods, left_on='BU_CODE', right_on='nbhood_code', how='left')\n",
    "\n",
    "gdf_selected_nbhoods['rand_points'] = gdf_selected_nbhoods.apply(lambda x: generate_coordinates(x['geometry'], 40), axis=1)\n",
    "\n",
    "#merge it with merged\n",
    "gdf_selected_nbhoods = gdf_selected_nbhoods.drop(['moderate_high_risk'],axis = 1)\n",
    "merged = pd.concat([merged, gdf_selected_nbhoods], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the locations of the sampled coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points_in_nbh(output_folder, gpd_df):\n",
    "    for i in range(len(gpd_df)):\n",
    "        # Get the geometry and rand_points for the selected index\n",
    "        geometry = gpd_df.loc[i, 'geometry']\n",
    "        rand_points = gpd_df.loc[i, 'rand_points']\n",
    "\n",
    "        rand_points = gpd.GeoDataFrame({'geometry': rand_points})\n",
    "\n",
    "        #Title for the plot\n",
    "        high = gpd_df.high_risk.iloc[i]\n",
    "        level = gpd_df.level_high.iloc[i]\n",
    "        name_nbh = gpd_df.BU_NAAM.iloc[i]\n",
    "        munic = gpd_df.GM_NAAM.iloc[i]\n",
    "        t = \"GSV locations for \"+name_nbh+\" in \"+munic+\", \\nhigh_risk \"+str(high)+\", level \"+str(level)\n",
    "\n",
    "        #this is for y-axis tick labels formatting\n",
    "        def format_decimal(x, pos=None):\n",
    "            return f\"{x:.3f}\"\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        gpd_df.iloc[[i]].plot(ax=ax, color='#CCCCCC', edgecolor='black')\n",
    "        rand_points.plot(ax=ax, color='red', markersize=5)\n",
    "        plt.title(t)\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(format_decimal))\n",
    "       \n",
    "        image_name = f\"image_{i}.png\"  # You can customize the naming\n",
    "        image_path = os.path.join(output_folder, image_name)\n",
    "        plt.savefig(image_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "#extract lat-lon coordinates\n",
    "def points_to_lat_lon(points_list):\n",
    "    return [(point.y, point.x) for point in points_list]\n",
    "\n",
    "gdf_df['lat_lon_points'] = gdf_df['rand_points'].apply(points_to_lat_lon)\n",
    "output_folder = 'some_path_for_plots' #write the path here!!\n",
    "plot_points_in_nbh(output_folder, gdf_df[:5]) #plots points for the first 5 nbhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the images, using the sampled coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([first, second, third, fourth], ignore_index=True)\n",
    "all_data.to_pickle(\"all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_streetview_image(latitude, longitude, api_key, save_path, file_name, size=\"512x512\"):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/streetview?size={size}&location={latitude},{longitude}&key={api_key}\"\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(save_path, file_name), \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "save_path = \"./nl_svi\"  # Replace with your desired save path\n",
    "targets = []\n",
    "count = 0\n",
    "\n",
    "for _, row in all_data.iterrows(): \n",
    "    risk = row.high_risk\n",
    "    level_high = row.level_high\n",
    "    nbhood = row.nbhood_code\n",
    "    for p in row.rand_points:\n",
    "        latitude = p.y\n",
    "        longitude = p.x\n",
    "        filename = str(count)+\"_\"+str(round(latitude,4))+\"_\"+str(round(longitude,4))+\".jpg\"\n",
    "        ID = f\"{count}_{latitude:.10f}_{longitude:.10f}\"\n",
    "        download_streetview_image(latitude, longitude, api_key, save_path, filename)\n",
    "        targets.append({'ID':ID, 'file_name': filename, 'target': risk, 'discrete_target':level_high, 'nbhood':nbhood})\n",
    "        count += 1\n",
    "\n",
    "targets = pd.DataFrame(targets)\n",
    "targets['discrete_target'] = targets['discrete_target'] - 1 #for classifier it's better this way\n",
    "display(targets)\n",
    "targets.to_csv('targets_and_coords.csv', index=False)\n",
    "\n",
    "#print counts per level:\n",
    "counts = targets['discrete_target'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK1DhFQOlun8"
   },
   "source": [
    "# For continuous targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x8ZeaqsKoxb"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnazFRuIzwHZ"
   },
   "source": [
    "CustomDataset class that is adjusted to load the images with continuous targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cY4zJii_zCnK"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, img_folder, transform=None):\n",
    "        self.df = df\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        self.min_target = self.df['high_risk'].min()\n",
    "        self.max_target = self.df['high_risk'].max()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.df.iloc[index]['file_name']\n",
    "        img_path = os.path.join(self.img_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        target = self.df.iloc[index, 4]\n",
    "        #target_normalized = (target - self.min_target) / (self.max_target - self.min_target) #can also try this\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(target, dtype=torch.float) #torch.tensor(target_normalized, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0pgq4i_z-Q6"
   },
   "outputs": [],
   "source": [
    "# Define the data transforms necessary for the network that's being used\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA5WjLP8z5f-"
   },
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "df = pd.read_csv('disp_coords.csv')\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Calculate sizes of each split\n",
    "train_size = int(len(df) * 0.7) #set to less for hyperparameter selection\n",
    "val_size = int(len(df) * 0.15)\n",
    "\n",
    "# Split the data\n",
    "train_df, val_df, test_df = np.split(df, [train_size, train_size + val_size])\n",
    "\n",
    "# Save the indices\n",
    "torch.save(train_df.index.values, 'train_indices_continuous_tiny.pt')\n",
    "torch.save(val_df.index.values, 'val_indices_continuous_tiny.pt')\n",
    "torch.save(test_df.index.values, 'test_indices_continuous_tiny.pt')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_df, './nl_svi_dispersed_green', transform=train_transforms) #set this to train_transforms for random crop\n",
    "val_dataset = CustomDataset(val_df, './nl_svi_dispersed_green', transform=val_transforms)\n",
    "test_dataset = CustomDataset(test_df, './nl_svi_dispersed_green', transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdnaVwaG7Gwk"
   },
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQ4mBO7h7I8q"
   },
   "outputs": [],
   "source": [
    "def train_custom_model(model_name, unfreeze_layers, optimizer_fn, lr, dropout_par, num_epochs, *args, **kwargs):\n",
    "    model = torch.hub.load('facebookresearch/deit:main', model_name, pretrained=True)\n",
    "\n",
    "    # Get the number of transformer layers\n",
    "    num_transformer_layers = len(model.blocks)\n",
    "\n",
    "    # Freeze all layers except the specified number of unfrozen layers\n",
    "    for i in range(num_transformer_layers - unfreeze_layers):\n",
    "        for param in model.blocks[i].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    num_classes = 1\n",
    "    in_features = model.head.in_features\n",
    "\n",
    "    model.head = nn.Sequential(\n",
    "        nn.Dropout(dropout_par),\n",
    "        nn.BatchNorm1d(num_features=in_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_par),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    #print(\"Model built.\")\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer_fn(model.parameters(), lr=lr, *args, **kwargs)\n",
    "\n",
    "    # Define the learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    early_stopping_patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    #custom metric to discretize the performance measurement\n",
    "    how_close = 2.6 #it's SD for our data, you can try different thresholds\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0.0\n",
    "        train_approx_right = 0\n",
    "\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Calculate the custom metric\n",
    "            close_enough = torch.abs(outputs - labels.float()) <= how_close\n",
    "            train_approx_right += torch.sum(close_enough).item()\n",
    "\n",
    "        # Calculate average loss, MAE and MSE over one epoch\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_approx_acc = train_approx_right / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        val_approx_right = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # We won't need gradients for validation, so save memory and computation\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs).squeeze()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels.float())\n",
    "\n",
    "                # Update loss and acc\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                close_enough = torch.abs(outputs - labels.float()) <= how_close\n",
    "                val_approx_right += torch.sum(close_enough).item()\n",
    "\n",
    "        # Calculate average loss and acc over validation dataset\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_approx_acc = val_approx_right / len(val_loader.dataset)\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Print validation statistics\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
    "        Approx Train Acc: {train_approx_acc:.4f}, Avg MSE Train: {train_loss:.4f}, \\\n",
    "        Approx Val Acc: {val_approx_acc:.4f}, Avg MSE Val: {val_loss:.4f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered, stopping at epoch\", epoch)\n",
    "            break\n",
    "    #return model #uncomment for full fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4XLe1fGAAQR"
   },
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYj-ge-LwW-N"
   },
   "outputs": [],
   "source": [
    "m_names = ['deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224']\n",
    "dropouts = [0, 0.2, 0.5, 0.7]\n",
    "to_unfreeze = [0,1,3,5]\n",
    "lr_s = [0.001, 0.005, 0.01]\n",
    "optimizers = [optim.Adam, optim.SGD, optim.Adagrad]\n",
    "\n",
    "for m_n in m_names:\n",
    "    for drop in dropouts:\n",
    "        for to_unfr in to_unfreeze:\n",
    "            for lr in lr_s:\n",
    "                for opt in optimizers:\n",
    "                    print(m_n, \"dropout:\", drop, \"layers:\", to_unfr, \"lr =\", lr, str(opt), end='\\n\\n')\n",
    "                    train_custom_model(m_n, to_unfr, opt, lr, drop, 15)\n",
    "                    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U41Nkm3yuGTf"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otj3ee8puImt"
   },
   "source": [
    "Choose the model that shows the most progress, based on the previous section. Change the percenetage of the data used (in the Loading data section). Fill in the hyperparameters below, accounting for more data if necessary (increase dropout, L2 regularization,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuK1zZ7LvBOT"
   },
   "outputs": [],
   "source": [
    "model = train_custom_model('deit_base_patch16_224', 5, optim.Adagrad, 0.001, 0.2, 100, weight_decay=3e-4)\n",
    "torch.save(model.state_dict(), 'deit_continuous.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9BzXT5_R58M"
   },
   "source": [
    "# For discretized targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hya1mLBsWD8m"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aT73UsV-WDVO"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, img_folder, transform=None):\n",
    "        self.df = df\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.df.iloc[index]['file_name']\n",
    "        img_path = os.path.join(self.img_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        target = self.df.iloc[index]['discrete_target'] #use \"discrete_target\" for 4 bins data\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSUA1k6FWQPw"
   },
   "outputs": [],
   "source": [
    "# Define the data transforms necessary for the network that's being used\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5VKXEDNWUN3"
   },
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "df = pd.read_csv('green_disp_coords_orig_bins.csv')\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Calculate sizes of each split\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.15)\n",
    "\n",
    "# Split the data\n",
    "train_df, val_df, test_df = np.split(df, [train_size, train_size + val_size])\n",
    "\n",
    "# Save the indices\n",
    "torch.save(train_df.index.values, 'train_indices_orig_bins.pt')\n",
    "torch.save(val_df.index.values, 'val_indices_orig_bins.pt')\n",
    "torch.save(test_df.index.values, 'test_indices_orig_bins.pt')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_df, './nl_svi_dispersed_green', transform=train_transforms) #set this to train_transforms for random crop\n",
    "val_dataset = CustomDataset(val_df, './nl_svi_dispersed_green', transform=val_transforms)\n",
    "test_dataset = CustomDataset(test_df, './nl_svi_dispersed_green', transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dA7UK6IVydw"
   },
   "source": [
    "## DeiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFr0Mtk5OADL"
   },
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3ML3zUVSIij"
   },
   "outputs": [],
   "source": [
    "def train_custom_model(model_name, unfreeze_layers, optimizer_fn, lr, dropout_par, num_epochs, *args, **kwargs):\n",
    "    model = torch.hub.load('facebookresearch/deit:main', model_name, pretrained=True)\n",
    "\n",
    "    # Get the number of transformer layers\n",
    "    num_transformer_layers = len(model.blocks)\n",
    "\n",
    "    # Freeze all layers except the specified number of unfrozen layers\n",
    "    for i in range(num_transformer_layers - unfreeze_layers):\n",
    "        for param in model.blocks[i].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    num_classes = 4\n",
    "    in_features = model.head.in_features\n",
    "\n",
    "    model.head = nn.Sequential(\n",
    "        nn.Dropout(dropout_par),\n",
    "        nn.BatchNorm1d(num_features=in_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_par),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    #print(\"Model built.\")\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer_fn(model.parameters(), lr=lr, *args, **kwargs)\n",
    "\n",
    "    # Define the learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    early_stopping_patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #print(\"Start epoch.\")\n",
    "        # Training\n",
    "        train_loss, train_correct, train_total, train_f1_total = 0.0, 0, 0, 0\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "            train_total += labels.size(0)\n",
    "            train_f1_total += f1_score(labels.cpu().detach(), preds.cpu().detach(), average='weighted') * labels.size(0)\n",
    "\n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct.item() / train_total\n",
    "        train_f1 = train_f1_total / train_total\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_correct, val_total, val_f1_total = 0.0, 0, 0, 0\n",
    "        model.eval()\n",
    "        #print(\"Started validation.\")\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "                #if i%10 == 0 and i !=0:\n",
    "                 # print(\"Batch\", i)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "                val_total += labels.size(0)\n",
    "                val_f1_total += f1_score(labels.cpu().detach(), preds.cpu().detach(), average='weighted') * labels.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct.item() / val_total\n",
    "        val_f1 = val_f1_total / val_total\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}],\\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered, stopping at epoch\", epoch)\n",
    "            break\n",
    "    return model #comment this line for hyperparameter selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBTcPVGyHR7A"
   },
   "source": [
    "### Hyperparameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcsSVBCeHYuA"
   },
   "source": [
    "Below is the code to perform grid search. The hyperparameters include three types of the DeiT models, amount of layers to re-train, learning rates and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFsWiWAxHUdj"
   },
   "outputs": [],
   "source": [
    "m_names = ['deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224']\n",
    "to_unfreeze = [0,1,3,5]\n",
    "lr_s = [0.001, 0.005, 0.01]\n",
    "optimizers = [optim.Adam, optim.SGD, optim.Adagrad]\n",
    "\n",
    "for m_n in m_names:\n",
    "  for to_unfr in to_unfreeze:\n",
    "    for lr in lr_s:\n",
    "      for opt in optimizers:\n",
    "        print(m_n, to_unfr, lr, str(opt), end='\\n\\n')\n",
    "        train_custom_model(m_n, to_unfr, opt, lr, 0)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oBZzwqmODsJ"
   },
   "source": [
    "### Train top-1 model with regularization and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHvlm89MWWMa"
   },
   "outputs": [],
   "source": [
    "model = train_custom_model('deit_base_patch16_224', 5, optim.SGD, 0.001, 0.2, 100, weight_decay=1e-4)\n",
    "torch.save(model.state_dict(), 'base_5_SGD_001_reg.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX1yYtYLOTLB"
   },
   "source": [
    "### For experiment with full landscapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn956Nc2cVlm"
   },
   "source": [
    "Code to conduct an experiment where the model is trained on the full landscapes, so the images are resized to the dimensions expected by the model and no random crop is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9fLRZjVOWWI"
   },
   "outputs": [],
   "source": [
    "#this assumes you have already run the code above and thus only have to re-define the train part\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_df, './nl_svi_dispersed_green', transform=val_transforms) #set this to val_transforms to avoid random crop\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXVXn6L8yzk2"
   },
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDlggVL2OY50"
   },
   "outputs": [],
   "source": [
    "m_names = ['deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224']\n",
    "to_unfreeze = [0,1,3,5]\n",
    "lr_s = [0.001, 0.01]\n",
    "optimizers = [optim.Adam, optim.SGD, optim.RMSprop, optim.Adagrad]\n",
    "dropouts = [0, 0.2, 0.4]\n",
    "L2s = [0,1e-3]\n",
    "\n",
    "for to_unfr in to_unfreeze:\n",
    "  print(\"\\n{} LAYERS UNFROZEN\\n\".format(to_unfr))\n",
    "  for m_n in m_names:\n",
    "    for lr in lr_s:\n",
    "      for opt in optimizers:\n",
    "        for do in dropouts:\n",
    "          for w_d in L2s:\n",
    "            print(m_n, to_unfr, str(opt), lr, do, w_d, end='\\n\\n')\n",
    "            train_custom_model(m_n, to_unfr, opt, lr, do, 15, weight_decay=w_d)\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84SPSSd8OG38"
   },
   "source": [
    "### To load later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-06EzgFRCppY"
   },
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(pd.read_csv('green_dispersed_coords.csv'), './nl_svi_dispersed_green', transform=val_transforms)\n",
    "\n",
    "# Load the indices (when resuming)\n",
    "#train_indices = torch.load('train_indices_final.pt')\n",
    "#val_indices = torch.load('val_indices_final.pt')\n",
    "test_indices = torch.load('test_indices_resnet.pt')\n",
    "\n",
    "# Create the datasets using the loaded indices\n",
    "#train_dataset = Subset(custom_dataset, train_indices)\n",
    "#val_dataset = Subset(custom_dataset, val_indices)\n",
    "test_dataset = Subset(custom_dataset, test_indices)\n",
    "\n",
    "# Create data loaders\n",
    "#train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKX_cCDWFrSH"
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=False)\n",
    "\n",
    "# Get the number of transformer layers\n",
    "num_transformer_layers = len(model.blocks)\n",
    "\n",
    "# Freeze all layers except the specified number of unfrozen layers\n",
    "for i in range(num_transformer_layers - 5):\n",
    "    for param in model.blocks[i].parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "num_classes = 4\n",
    "in_features = model.head.in_features\n",
    "\n",
    "model.head = nn.Sequential(\n",
    "    nn.Dropout(),\n",
    "    nn.BatchNorm1d(num_features=in_features),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features, num_classes)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('deit_on_resnet_sets.pth')) #and the other one is 'deit_on_resnet_sets.pth'\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKGInk1bOKYK"
   },
   "source": [
    "### Evaluation and preprocessing for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFvNjD3iUQUa"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_correct, test_correct_adj, test_total, test_f1_total = 0.0, 0, 0, 0, 0\n",
    "\n",
    "model.eval()\n",
    "num_batches = len(test_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_correct += torch.sum(preds == labels.data)\n",
    "        test_correct_adj += torch.sum(preds == labels.data)\n",
    "        test_correct_adj += torch.sum( (preds +1) == labels.data) #to compute adjusted acc, comment this for standard acc\n",
    "        test_correct_adj += torch.sum( (preds -1) == labels.data) #to compute adjusted acc, comment this for standard acc\n",
    "        test_total += labels.size(0)\n",
    "        test_f1_total += f1_score(labels.cpu().detach(), preds.cpu().detach(), average='weighted') * labels.size(0)\n",
    "\n",
    "test_loss /= num_batches\n",
    "test_acc = test_correct.item() / test_total\n",
    "test_acc_adj = test_correct_adj.item() / test_total\n",
    "test_f1 = test_f1_total / test_total\n",
    "\n",
    "print('Test performance:')\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.5f}, Test Adj Acc: {test_acc_adj:.5f}, Test F1: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njiN9qDWUdva"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def imshow(img):\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225])[:, None, None] + torch.tensor([0.485, 0.456, 0.406])[:, None, None]  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(30, 3))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_correct, test_total, test_f1_total = 0.0, 0, 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for batch_index, (inputs, labels) in enumerate(test_loader):\n",
    "    if batch_index == 0:  # Only take the first batch\n",
    "        inputs, labels = inputs[10:20].to(device), labels[10:20].to(device)  # Select the first 10 examples\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_correct += torch.sum(preds == labels.data)\n",
    "        test_total += labels.size(0)\n",
    "        test_f1_total += f1_score(labels.cpu().detach(), preds.cpu().detach(), average='weighted') * labels.size(0)\n",
    "        print('preds:', preds)\n",
    "        print('labels:',labels.data)\n",
    "\n",
    "        inputs_cpu = inputs.cpu()\n",
    "        imshow(torchvision.utils.make_grid(inputs_cpu, nrow=10))\n",
    "    else:\n",
    "      break\n",
    "\n",
    "test_acc = test_correct.item() / test_total\n",
    "test_f1 = test_f1_total / test_total\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VdvLXeyw_IP"
   },
   "source": [
    "## ResNet50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_ok-lQhNzmj"
   },
   "source": [
    "### Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiGPZThvxDI1"
   },
   "outputs": [],
   "source": [
    "def train_custom_model_resnet50(unfreeze_layers, optimizer_fn, lr, **kwargs):\n",
    "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1) # Load ResNet50 instead of DeiT\n",
    "\n",
    "    # Get the number of layers in the ResNet50 model\n",
    "    num_layers = len(list(model.children()))\n",
    "\n",
    "    # Freeze all layers except the specified number of unfrozen layers\n",
    "    for i, child in enumerate(model.children()):\n",
    "        if i < num_layers - unfreeze_layers:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    num_classes = 4\n",
    "    in_features = model.fc.in_features  # Get the number of input features for the fully connected layer\n",
    "\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.BatchNorm1d(num_features=in_features),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer_fn(model.parameters(), lr=lr, **kwargs)\n",
    "\n",
    "    # Define the learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    early_stopping_patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    n_batch_train = len(train_loader)\n",
    "    n_batch_val = len(val_loader)\n",
    "\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_correct, train_total, train_f1_total = 0.0, 0, 0, 0\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "            train_total += labels.size(0)\n",
    "            train_f1_total += f1_score(labels.cpu().detach(), preds.cpu().detach(), average='weighted') * labels.size(0)\n",
    "\n",
    "        train_loss /= n_batch_train\n",
    "        train_acc = train_correct.item() / train_total\n",
    "        train_f1 = train_f1_total / train_total\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_correct, val_total, val_f1_total = 0.0, 0, 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "                val_total += labels.size(0)\n",
    "                val_f1_total += f1_score(labels.cpu().detach(), preds.cpu().detach(), average='weighted') * labels.size(0)\n",
    "\n",
    "        val_loss /= n_batch_val\n",
    "        val_acc = val_correct.item() / val_total\n",
    "        val_f1 = val_f1_total / val_total\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}],\\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered, stopping at epoch\", epoch)\n",
    "            break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnUUHjSzR78k"
   },
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tSO3v82Qq24"
   },
   "outputs": [],
   "source": [
    "to_unfreeze = [0,1,3,5]\n",
    "lr_s = [0.001, 0.005, 0.01]\n",
    "optimizers = [optim.Adam, optim.SGD, optim.RMSprop, optim.Adagrad]\n",
    "\n",
    "for to_unfr in to_unfreeze:\n",
    "  for lr in lr_s:\n",
    "    for opt in optimizers:\n",
    "      print(\"Training ResNet50 with {} last layers unfrozen, lr {}, {}\".format(to_unfr, lr, str(opt)), end='\\n\\n')\n",
    "      train_custom_model_resnet50(to_unfr, opt, lr)\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrE4Oz-GSCFI"
   },
   "source": [
    "### Actual model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmkxo2QmXSUL"
   },
   "outputs": [],
   "source": [
    "model = train_custom_model_resnet50(3, optim.Adagrad,  0.001, weight_decay=1e-4)\n",
    "torch.save(model.state_dict(), 'resnet_3_001_Adam_reg_deit_sets.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyiWFHZ5N56S"
   },
   "source": [
    "### Load model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u4IiGuLj02r"
   },
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(pd.read_csv('green_dispersed_coords.csv'), './nl_svi_dispersed_green', transform=val_transforms)\n",
    "\n",
    "# Load the indices (when resuming)\n",
    "#train_indices = torch.load('test_indices_bins1_resnet.pt')\n",
    "val_indices = torch.load('val_indices_bins1_resnet.pt')\n",
    "test_indices = torch.load('test_indices_resnet.pt')\n",
    "\n",
    "# Create the datasets using the loaded indices\n",
    "#train_dataset = Subset(custom_dataset, train_indices)\n",
    "val_dataset = Subset(custom_dataset, val_indices)\n",
    "test_dataset = Subset(custom_dataset, test_indices)\n",
    "\n",
    "# Create data loaders\n",
    "#train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaQMlygDOBLS"
   },
   "outputs": [],
   "source": [
    "unfreeze_layers = 3\n",
    "num_classes = 4\n",
    "\n",
    "#Load the model and adjust the architecture\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1) # Load ResNet50 instead of DeiT\n",
    "\n",
    "num_layers = len(list(model.children())) # Get the number of layers in the ResNet50 model\n",
    "\n",
    "for i, child in enumerate(model.children()): # Freeze all layers except the specified number of unfrozen layers\n",
    "    if i < num_layers - unfreeze_layers:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "in_features = model.fc.in_features  # Get the number of input features for the fully connected layer\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0),\n",
    "    nn.BatchNorm1d(num_features=in_features),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0),\n",
    "    nn.Linear(in_features, num_classes)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "#Load the weights\n",
    "model.load_state_dict(torch.load(\"resnet_3_Adagrad_01_reg.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ7iwwCIAUra"
   },
   "source": [
    "### Evaluation on the test set\n",
    "\n",
    "We use the test set and the random prediction here for comparison, and we then visualize the images for all these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wqLpva7Aef8"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_correct, test_correct_adj, test_total, test_f1_total = 0.0, 0, 0, 0, 0\n",
    "model.eval()\n",
    "num_batches = len(test_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_correct += torch.sum(preds == labels.data)\n",
    "        test_correct_adj += torch.sum(preds == labels.data)\n",
    "        test_correct_adj += torch.sum( (preds +1) == labels.data) #to compute adjusted acc, comment this for standard acc\n",
    "        test_correct_adj += torch.sum( (preds -1) == labels.data) #to compute adjusted acc, comment this for standard acc\n",
    "        test_total += labels.size(0)\n",
    "        test_f1_total += f1_score(labels.cpu().detach(), preds.cpu().detach(), average='weighted') * labels.size(0)\n",
    "\n",
    "test_loss /= num_batches\n",
    "test_acc = test_correct.item() / test_total\n",
    "test_acc_adj = test_correct_adj.item() / test_total\n",
    "test_f1 = test_f1_total / test_total\n",
    "\n",
    "print('Test performance of ResNet50')\n",
    "print(f'Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.5f}, Test Adj Acc: {test_acc_adj:.5f}, Test F1: {test_f1:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGIDAbiaGjDN"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def imshow(img):\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225])[:, None, None] + torch.tensor([0.485, 0.456, 0.406])[:, None, None]  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(30, 3))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_correct, test_total, test_f1_total = 0.0, 0, 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for batch_index, (inputs, labels) in enumerate(test_loader):\n",
    "    if batch_index == 0:  # Only take the first batch\n",
    "        inputs, labels = inputs[:10].to(device), labels[:10].to(device)  # Select the first 10 examples\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_correct += torch.sum(preds == labels.data)\n",
    "        test_total += labels.size(0)\n",
    "        test_f1_total += f1_score(labels.cpu().detach(), preds.cpu().detach(), average='weighted') * labels.size(0)\n",
    "        print('preds:', preds)\n",
    "        print('labels:',labels.data)\n",
    "\n",
    "        inputs_cpu = inputs.cpu()\n",
    "        imshow(torchvision.utils.make_grid(inputs_cpu, nrow=10))\n",
    "    else:\n",
    "      break\n",
    "\n",
    "test_loss /= test_total\n",
    "test_acc = test_correct.item() / test_total\n",
    "test_f1 = test_f1_total / test_total\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttar75WxoD31"
   },
   "source": [
    "# Explaining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5ji0TXANfcu"
   },
   "source": [
    "## Maybe useful plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNLSoiz0O7px"
   },
   "outputs": [],
   "source": [
    "for batch_index, (i, l) in enumerate(test_loader):\n",
    "    if batch_index == 0:  # Only take the first batch\n",
    "        inputs, labels = i[:10].to(device), l[:10].to(device)  # Select the first 10 examples\n",
    "        print(len(inputs))\n",
    "        inp = inputs[1]\n",
    "        lab = labels[1]\n",
    "    else:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9I0AokHPr_L"
   },
   "source": [
    "### To print one image or a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf-8QOr6Okiq"
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "def showGrid(img):\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225])[:, None, None] + torch.tensor([0.485, 0.456, 0.406])[:, None, None]  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(30, 3))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def showIm(img):\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225])[:, None, None] + torch.tensor([0.485, 0.456, 0.406])[:, None, None]  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOKjQ8b-P6xp"
   },
   "outputs": [],
   "source": [
    "print(labels)\n",
    "inputs_cpu = inputs.cpu()\n",
    "showGrid(torchvision.utils.make_grid(inputs_cpu, nrow=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQWmeki0PTN5"
   },
   "outputs": [],
   "source": [
    "print(\"Label:\", lab)\n",
    "inp = inp.to('cpu')\n",
    "showIm(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj8ds0lUPqfY"
   },
   "source": [
    "## Getting the top predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjdVcuRhpojU"
   },
   "outputs": [],
   "source": [
    "def get_extreme_indices():\n",
    "    extreme_indices = []\n",
    "    logits = []\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions = (preds == labels.data)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            for i, is_correct in enumerate(correct_predictions):\n",
    "                if is_correct:\n",
    "                    # Compute the index of the sample in the dataset\n",
    "                    sample_index = batch_index * test_loader.batch_size + i\n",
    "                    correct_class = labels[i].item()\n",
    "                    # Store the index, correct class, and corresponding logit in a tuple\n",
    "                    correct_logit = outputs[i, labels[i]].item()\n",
    "                    extreme_indices.append((sample_index, correct_class, correct_logit))\n",
    "                    # Store the logit corresponding to the correct prediction\n",
    "                    logits.append(correct_logit)\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Sort the tuples based on the most extreme logits\n",
    "    sorted_tuples = sorted(extreme_indices, key=lambda x: abs(x[2]), reverse=True)\n",
    "    return sorted_tuples, cm\n",
    "\n",
    "model.eval()\n",
    "extreme_tuples, cm = get_extreme_indices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHz0On19tLnR"
   },
   "outputs": [],
   "source": [
    "print(extreme_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sf09I4MJuSfq"
   },
   "outputs": [],
   "source": [
    "with open('extreme_deit_deit.pkl', 'wb') as f:\n",
    "    pickle.dump(extreme_tuples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCEDSPSmg4pB"
   },
   "outputs": [],
   "source": [
    "with open('extreme_resnet.pkl', 'rb') as f:\n",
    "    extreme_tuples = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgCjodxNWD8T"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, name):\n",
    "    tick_names = ['very low', 'low', 'moderate', 'high&v. high']\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16})  # Increase label size here\n",
    "    plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5], labels=tick_names, fontsize=14)  # Set x-axis tick names\n",
    "    plt.yticks(ticks=[0.5, 1.5, 2.5, 3.5], labels=tick_names, fontsize=14)  # Set y-axis tick names\n",
    "    plt.xlabel('Predicted', fontsize=20)  # Increase x-axis label size\n",
    "    plt.ylabel('Truth', fontsize=20)  # Increase y-axis label size\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    plt.tight_layout()  # Remove margins\n",
    "    plt.savefig(name + '.eps', bbox_inches='tight')  # Save with tight bounding box\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njmsKSAZWGc1"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm,'conf_deit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuCsii3GqwZM"
   },
   "outputs": [],
   "source": [
    "def get_top_images_per_category(indices, top_n=10):\n",
    "    top_images = {i: [] for i in range(4)}\n",
    "    for index, class_num, logit in indices:\n",
    "        if len(top_images[class_num]) < top_n:\n",
    "            top_images[class_num].append(index)\n",
    "        if all(len(images) >= top_n for images in top_images.values()):\n",
    "            break\n",
    "    return top_images\n",
    "\n",
    "top_n = 10\n",
    "top_images_per_category = get_top_images_per_category(extreme_tuples, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNVSXhGZegmU"
   },
   "outputs": [],
   "source": [
    "for class_num, image_indices in top_images_per_category.items():\n",
    "    print(f\"Top {top_n} images for class {class_num}:\")\n",
    "    for index in image_indices:\n",
    "        image_t, target = test_dataset[index]\n",
    "        showIm(image_t)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qangR5B0PlKg"
   },
   "source": [
    "## SHAP\n",
    "\n",
    "https://github.com/shap/shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvaquNBnPxWI"
   },
   "outputs": [],
   "source": [
    "#just like with timm, you need to install this every time when you work in the cloud\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oRzDaw1YO4k"
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhlbWh5pqSGB"
   },
   "outputs": [],
   "source": [
    "for class_num, image_indices in top_images_per_category.items():\n",
    "  print(class_num)\n",
    "  print(image_indices)\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-NSSnVqeLDY"
   },
   "outputs": [],
   "source": [
    "to_explain= []\n",
    "\n",
    "for class_num, image_indices in top_images_per_category.items():\n",
    "    for index in image_indices:\n",
    "        image_t, target = test_dataset[index]\n",
    "        to_explain.append(image_t)\n",
    "\n",
    "to_explain = torch.stack(to_explain).cuda()\n",
    "\n",
    "def normalize(tensor):\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    return (tensor - tensor_min) / (tensor_max - tensor_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OXT3dpPZFoL"
   },
   "outputs": [],
   "source": [
    "class_names = [\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"]\n",
    "model.eval()\n",
    "\n",
    "# Use GradientExplainer\n",
    "background_indices = np.random.choice(len(test_dataset), 50, replace=False)\n",
    "background = torch.stack([test_dataset[i][0] for i in background_indices]).cuda()\n",
    "e = shap.GradientExplainer(model, background)\n",
    "\n",
    "shap_values,indexes = e.shap_values(to_explain, ranked_outputs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QNfulu5Fh-de"
   },
   "outputs": [],
   "source": [
    "# Get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[x])(indexes.cpu())\n",
    "\n",
    "#prepare the data to comply with the expectations of the plotting function\n",
    "to_explain = to_explain.permute(0, 2, 3, 1)\n",
    "shap_values = [np.transpose(sv, (0, 2, 3, 1)) for sv in shap_values]\n",
    "to_explain_norm = normalize(to_explain)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain_norm.cpu().numpy(), index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EVIZO2hT6_Cl"
   },
   "outputs": [],
   "source": [
    "# To save\n",
    "torch.save(shap_values, 'shap_values_resnet_4.pt')\n",
    "torch.save(indexes, 'indexes_resnet_4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSCR3TB07BJ4"
   },
   "outputs": [],
   "source": [
    "# To load\n",
    "shap_values = torch.load('shap_values.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6LAyZHWBYkB"
   },
   "source": [
    "## Gradient rollout\n",
    "\n",
    "Gradient rollout was performed on the local CPU, not in the cloud. The code below would work on a PC, but not on cloud service.\n",
    "\n",
    "**The code assumes you cloned Gildenblat's repository. Before running the code, go to ./vit-explain , inside the functions, and change the path to the model to the path where you saved the trained model, and re-create the model there if you only saved  weights (like this code generally does). There is code for re-creating the model in this notebook in sub-section DeiT/To load later. If you adjusted anything in the architecture of the model, it should also be adjusted there where it re-builds the model before it loads the weights.**\n",
    "\n",
    "The application of Gradient Rollout to DeiT models is courtesy of J. Gildenblat. https://jacobgil.github.io/deeplearning/vision-transformer-explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9Qq8jM5D4OQ"
   },
   "outputs": [],
   "source": [
    "os.chdir('./vit-explain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acB9l2TLD441"
   },
   "outputs": [],
   "source": [
    "for class_num, dat in top_images_per_category.items():\n",
    "    print(f\"Top {top_n} images for class {class_num}\")\n",
    "    for index, logit in dat:\n",
    "        original_index = test_indices[index]\n",
    "        img_name = test_dataset.dataset.get_img_name(original_index)\n",
    "        img_name = '../nl_svi_dispersed_green/'+img_name\n",
    "        !python vit_explain.py --image_path {img_name} --head_fusion mean --discard_ratio 0.8 --logit {logit} --cl {class_num}\n",
    "        !python vit_explain.py --image_path {img_name} --head_fusion max --discard_ratio 0.8 --logit {logit} --cl {class_num}\n",
    "        !python vit_explain.py --image_path {img_name} --head_fusion min --discard_ratio 0.8 --logit {logit} --cl {class_num}\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl9yXeKsEMk3"
   },
   "source": [
    "### And with the least certain predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxL39RMsEQnY"
   },
   "outputs": [],
   "source": [
    "extreme_tuples.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbhfPDjYESOQ"
   },
   "outputs": [],
   "source": [
    "def get_bottom_images_per_category(indices, n=10):\n",
    "    b_images = defaultdict(list)\n",
    "    for index, class_num, logit in indices:\n",
    "        if len(b_images[class_num]) < n:\n",
    "            b_images[class_num].append((index, logit))\n",
    "\n",
    "    return b_images\n",
    "\n",
    "bottom_n = 10\n",
    "b_images_per_category = get_bottom_images_per_category(extreme_tuples,bottom_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z17aCIL4EXa3"
   },
   "outputs": [],
   "source": [
    "for class_num, dat in b_images_per_category.items():\n",
    "    print(f\"Bottom {bottom_n} images for class {class_num}\")\n",
    "    for index, logit in dat:\n",
    "        original_index = test_indices[index]\n",
    "        img_name = test_dataset.dataset.get_img_name(original_index)\n",
    "        img_name = '../nl_svi_green_10000/'+img_name\n",
    "        !python vit_explain.py --image_path {img_name} --head_fusion mean --discard_ratio 0.8 --logit {logit} --cl {class_num}\n",
    "        !python vit_explain.py --image_path {img_name} --head_fusion max --discard_ratio 0.8 --logit {logit} --cl {class_num}\n",
    "        !python vit_explain.py --image_path {img_name} --head_fusion min --discard_ratio 0.8 --logit {logit} --cl {class_num}\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "IOcXy2_xKkRH",
    "dK1DhFQOlun8",
    "8x8ZeaqsKoxb",
    "vdnaVwaG7Gwk",
    "X4XLe1fGAAQR",
    "U41Nkm3yuGTf",
    "T9BzXT5_R58M",
    "Hya1mLBsWD8m",
    "-dA7UK6IVydw",
    "QFr0Mtk5OADL",
    "nBTcPVGyHR7A",
    "2oBZzwqmODsJ",
    "bX1yYtYLOTLB",
    "MXVXn6L8yzk2",
    "84SPSSd8OG38",
    "YKGInk1bOKYK",
    "6VdvLXeyw_IP",
    "z_ok-lQhNzmj",
    "rnUUHjSzR78k",
    "BrE4Oz-GSCFI",
    "vyiWFHZ5N56S",
    "CJ7iwwCIAUra",
    "Ttar75WxoD31",
    "b5ji0TXANfcu",
    "xj8ds0lUPqfY",
    "qangR5B0PlKg",
    "f6LAyZHWBYkB",
    "Vl9yXeKsEMk3"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
